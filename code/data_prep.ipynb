{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "def make_id(name):\n",
    "    \"\"\"\n",
    "    This function combines two columns to make unique ids for each property,\n",
    "    to be used later for merging.\n",
    "    \"\"\"\n",
    "    name['Major'] = name['Major'].astype(str).str.zfill(6)\n",
    "    name['Minor'] = name['Minor'].astype(str).str.zfill(4)\n",
    "    name['id'] = name['Major'].str.cat(name['Minor'])\n",
    "    name.drop(columns=['Major', 'Minor'], inplace=True)\n",
    "    return name\n",
    "\n",
    "\n",
    "def original_prep(df,ods):\n",
    "    \"\"\"\n",
    "    This function cleans and prepares the original dataset.\n",
    "    \"\"\"\n",
    "    # Importing list of zipcodes from King County\n",
    "    kings_zips = list(ods['Zip Code'])\n",
    "\n",
    "    # Extracting the zipcode from the address column.\n",
    "    df['zipcode'] = df['address'].apply(lambda x: int(x[-20:-15]))\n",
    "    df = df[df['zipcode'].isin(kings_zips)]\n",
    "\n",
    "    # Making sure id's are 10 characters long and dropping duplicate id's.\n",
    "    df['id'] = df['id'].astype(str).str.zfill(10)\n",
    "    df.drop_duplicates(subset=['id'], inplace=True)\n",
    "\n",
    "    # Renaming elements in categorical variables to a binary 0 and 1, or a\n",
    "    # numerical ranking.\n",
    "    df['greenbelt'] = df['greenbelt'].map({'NO': 0, 'YES': 1})\n",
    "    df['nuisance'] = df['nuisance'].map({'NO': 0, 'YES': 1})\n",
    "    df['waterfront'] = df['waterfront'].map({'NO': 0, 'YES': 1})\n",
    "    df['view'] = df['view'].map({\n",
    "        'NONE': 0,\n",
    "        'POOR': 1,\n",
    "        'FAIR': 2,\n",
    "        'AVERAGE': 3,\n",
    "        'GOOD': 4,\n",
    "        'EXCELLENT': 5\n",
    "    })\n",
    "    df['condition'] = df['condition'].map({\n",
    "        'Poor': 1,\n",
    "        'Fair': 2,\n",
    "        'Average': 3,\n",
    "        'Good': 4,\n",
    "        'Very Good': 5\n",
    "    })\n",
    "    df['grade'] = df['grade'].map({\n",
    "        '1 Cabin': 1,\n",
    "        '2 Substandard': 2,\n",
    "        '3 Poor': 3,\n",
    "        '4 Low': 4,\n",
    "        '5 Fair': 5,\n",
    "        '6 Low Average': 6,\n",
    "        '7 Average': 7,\n",
    "        '8 Good': 8,\n",
    "        '9 Better': 9,\n",
    "        '10 Very Good': 10,\n",
    "        '11 Excellent': 11,\n",
    "        '12 Luxury': 12,\n",
    "        '13 Mansion': 13\n",
    "    })\n",
    "\n",
    "    # Selecting columns to keep and setting the id as the index.\n",
    "    keep = [\n",
    "        'id', 'price', 'bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot',\n",
    "        'floors', 'waterfront', 'greenbelt', 'nuisance', 'view', 'condition',\n",
    "        'grade', 'sqft_basement', 'yr_built', 'yr_renovated', 'zipcode'\n",
    "    ]\n",
    "    df_clean = df[keep].set_index('id', verify_integrity=True)\n",
    "    return df_clean\n",
    "\n",
    "\n",
    "def res_prep(res,ods):\n",
    "    \"\"\"\n",
    "    This function cleans and prepares the residential dataset.\n",
    "    \"\"\"\n",
    "    make_id(res)  # Making 'id' column.\n",
    "\n",
    "    # Ensuring that all of the the zipcodes are in the proper format and filtering\n",
    "    # for zipcodes only in King County.\n",
    "    to_drop = []\n",
    "    kings_zips = list(ods['Zip Code'])\n",
    "    for i in res['ZipCode']:\n",
    "        try:\n",
    "            int(i[:5])\n",
    "        except:\n",
    "            if i in to_drop:\n",
    "                pass\n",
    "            else:\n",
    "                to_drop.append(i)\n",
    "    res.loc[res['ZipCode'].isin(to_drop), 'ZipCode'] = np.nan\n",
    "    res.dropna(subset=['ZipCode'], inplace=True)\n",
    "    res['zipcode'] = res['ZipCode'].apply(lambda x: int(str(x)[:5]))\n",
    "    res.loc[~res['zipcode'].isin(kings_zips), 'zipcode'] = np.nan\n",
    "    res.dropna(subset=['ZipCode'], inplace=True)\n",
    "\n",
    "    # Dropping duplicate id's.\n",
    "    res.drop_duplicates(subset=['id'], inplace=True, keep=False)\n",
    "\n",
    "    # Renaming columns to match the original dataset.\n",
    "    mapping = {\n",
    "        'Stories': 'floors',\n",
    "        'BldgGrade': 'grade',\n",
    "        'SqFtTotLiving': 'sqft_living',\n",
    "        'Bedrooms': 'bedrooms',\n",
    "        'BathFullCount': 'bathrooms',\n",
    "        'YrBuilt': 'yr_built',\n",
    "        'Condition': 'condition',\n",
    "        'SqFtTotBasement': 'sqft_basement',\n",
    "        'YrRenovated': 'yr_renovated'\n",
    "    }\n",
    "\n",
    "    # Selecting columns to keep and setting the id as the index.\n",
    "    keep = [\n",
    "        'id', 'bathrooms', 'bedrooms', 'condition', 'floors', 'grade',\n",
    "        'sqft_basement', 'sqft_living', 'yr_built', 'yr_renovated', 'zipcode'\n",
    "    ]\n",
    "    res_clean = res.rename(columns=mapping)[keep].set_index(\n",
    "        'id', verify_integrity=True)\n",
    "    return res_clean\n",
    "\n",
    "\n",
    "def parcel_prep(parcel):\n",
    "    \"\"\"\n",
    "    This function cleans and prepares the parcel dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    make_id(parcel)  # Making 'id' column.\n",
    "\n",
    "    # Filtering data to only include properties labeled as 'Condominium' or 'Residential'.\n",
    "    parcel = parcel[parcel['PropType'].isin(['K', 'R'])]\n",
    "\n",
    "    # Combining the information from multiple view columns to create a 'view'\n",
    "    # column which includes a view rating from 0 to 5.\n",
    "    for i in [\n",
    "            'MtRainier', 'Olympics', 'Cascades', 'Territorial',\n",
    "            'SeattleSkyline', 'PugetSound', 'LakeWashington', 'LakeSammamish',\n",
    "            'SmallLakeRiverCreek', 'OtherView'\n",
    "    ]:\n",
    "        parcel.loc[parcel[i] > 0, i] = 1\n",
    "        parcel.loc[parcel[i] == 0, i] = 0\n",
    "    parcel['total_views'] = parcel['MtRainier'] + parcel['Olympics'] + parcel[\n",
    "        'Cascades'] + parcel['Territorial'] + parcel[\n",
    "            'SeattleSkyline'] + parcel['PugetSound'] + parcel[\n",
    "                'LakeWashington'] + parcel['LakeSammamish'] + parcel[\n",
    "                    'SmallLakeRiverCreek'] + parcel['OtherView']\n",
    "    parcel['view'] = parcel['total_views'].map({\n",
    "        0: 0,\n",
    "        1: 2,\n",
    "        2: 3,\n",
    "        3: 4,\n",
    "        4: 5,\n",
    "        5: 5,\n",
    "        6: 5,\n",
    "        7: 5,\n",
    "        8: 5\n",
    "    })\n",
    "\n",
    "    # Renaming elements in categorical variables to a binary 0 and 1\n",
    "    parcel.loc[parcel['WfntLocation'] > 0, 'waterfront'] = 1\n",
    "    parcel.loc[parcel['WfntLocation'] == 0, 'waterfront'] = 0\n",
    "    parcel['nuisance'] = 0\n",
    "    parcel.loc[parcel['PowerLines'] == 'Y', 'nuisance'] = 1\n",
    "    parcel.loc[parcel['TrafficNoise'] > 0, 'nuisance'] = 1\n",
    "    parcel.loc[parcel['AirportNoise'] != 0, 'nuisance'] = 1\n",
    "    parcel.loc[parcel['OtherNuisances'] == 'Y', 'nuisance'] = 1\n",
    "    parcel['greenbelt'] = parcel['AdjacentGreenbelt'].map({'N': 0, 'Y': 1})\n",
    "\n",
    "    # Dropping duplicate id's.\n",
    "    parcel.drop_duplicates(subset=['id'], inplace=True, keep=False)\n",
    "\n",
    "    # Renaming column to match the original dataset.\n",
    "    mapping = {'SqFtLot': 'sqft_lot'}\n",
    "\n",
    "    # Selecting columns to keep and setting the id as the index.\n",
    "    keep = ['id', 'greenbelt', 'nuisance', 'sqft_lot', 'view', 'waterfront']\n",
    "    parcel_clean = parcel.rename(columns=mapping)[keep].set_index(\n",
    "        'id', verify_integrity=True)\n",
    "    return parcel_clean\n",
    "\n",
    "\n",
    "def sales_prep(sales):\n",
    "    \"\"\"\n",
    "    This function cleans and prepares the sales dataset.\n",
    "    \"\"\"\n",
    "    # In this case, the sales data had many id's in an inproper format. This code\n",
    "    # cleans and filter the ids\n",
    "    sales['Major'] = sales['Major'].astype(str)\n",
    "    sales['Minor'] = sales['Minor'].astype(str)\n",
    "    sales['id'] = sales['Major'].str.cat(sales['Minor'])\n",
    "    to_drop = []\n",
    "    for i, j in enumerate(sales['id']):\n",
    "        try:\n",
    "            int(j)\n",
    "        except:\n",
    "            to_drop.append(j)\n",
    "    sales.loc[sales['id'].isin(to_drop), 'id'] = np.nan\n",
    "    sales.dropna(subset=['id'], inplace=True)\n",
    "\n",
    "    # Converting and extracting the year from the date column.\n",
    "    sales['date'] = pd.to_datetime(sales['DocumentDate'])\n",
    "    sales['year'] = sales['date'].apply(lambda x: x.year)\n",
    "\n",
    "    # Selecting and renaming the columns.\n",
    "    keep = ['id', 'date', 'price']\n",
    "    mapping = {'SalePrice': 'price'}\n",
    "\n",
    "    cond1 = sales['id'] != 0\n",
    "\n",
    "    # Filter to include condominiums, apartments, residential buildings, and mobile homes.\n",
    "    cond2 = sales['PrincipalUse'].isin([2, 4, 6, 8])\n",
    "\n",
    "    cond3 = sales['year'] > 2020\n",
    "    sales2 = sales[cond1 & cond2 & cond3].rename(columns=mapping)[keep]\n",
    "\n",
    "    # Only keeping the most recent sale of a property.\n",
    "    sales3 = sales2.sort_values('date',\n",
    "                                ascending=False).drop_duplicates(subset=['id'],\n",
    "                                                                 keep='first')\n",
    "\n",
    "    sales_clean = sales3.sort_values('id').set_index('id',\n",
    "                                                     verify_integrity=True)\n",
    "    return sales_clean\n",
    "\n",
    "\n",
    "def ods_prep(ods):\n",
    "    \"\"\"\n",
    "    This function cleans and prepares the open datasoft dataset.\n",
    "    \"\"\"\n",
    "    # Selecting and renaming the columns.\n",
    "    ods_clean = ods[['Zip Code', 'Population', 'Density']]\n",
    "    ods_clean.rename(columns={\n",
    "        'Zip Code': 'zipcode',\n",
    "        'Population': 'population',\n",
    "        'Density': 'density'\n",
    "    },\n",
    "                     inplace=True)\n",
    "    return ods_clean\n",
    "\n",
    "\n",
    "def merge_prep(sales_clean, orig_clean, res_clean, parcel_clean, ods_clean):\n",
    "    \"\"\"\n",
    "    Merging, updating, cleaning, and filtering all of the datasets used in this project.\n",
    "    \"\"\"\n",
    "    # Merging all of the above datasets.\n",
    "    data_clean = sales_clean.join(orig_clean.iloc[:, 1:])\n",
    "    data_clean.update(res_clean)\n",
    "    data_clean.update(parcel_clean)\n",
    "    data_clean = data_clean.reset_index().merge(ods_clean,\n",
    "                                                how=\"left\",\n",
    "                                                on='zipcode').set_index(\n",
    "                                                    'id',\n",
    "                                                    verify_integrity=True)\n",
    "\n",
    "    # Filtering and removing all missing values.\n",
    "    data_clean.loc[data_clean['price'] <= 0, 'price'] = np.nan\n",
    "    data_clean.loc[data_clean['sqft_living'] <= 0, 'sqft_living'] = np.nan\n",
    "    data_clean.dropna(subset=['price', 'sqft_living', 'zipcode'], inplace=True)\n",
    "\n",
    "    # Excluding outliers by selecting for the middle 95% price and sqft_living data.\n",
    "    data_clean['price_nlog'] = (np.log(data_clean['price']) - np.log(\n",
    "        data_clean['price']).mean()) / np.log(data_clean['price']).std()\n",
    "    data_clean.loc[(data_clean['price_nlog'] > 2) |\n",
    "                   (data_clean['price_nlog'] < -2), 'price_nlog'] = np.nan\n",
    "    data_clean.dropna(subset=['price_nlog'], inplace=True)\n",
    "\n",
    "    # Creating a column to calculate the last year construction was done on the property.\n",
    "    data_clean['yr_last_construction'] = data_clean['yr_built']\n",
    "    data_clean['yr_last_construction'].update(\n",
    "        data_clean['yr_renovated'][data_clean['yr_renovated'] != 0])\n",
    "\n",
    "    data_clean = data_clean[data_clean['bedrooms'].between(1, 6)]\n",
    "    data_clean = data_clean[data_clean['bathrooms'].between(1, 6)]\n",
    "    data_clean = data_clean[(data_clean['sqft_lot'] < 43560)]\n",
    "    data_clean = data_clean[data_clean['sqft_basement'] < 4000]\n",
    "\n",
    "    # Excluding zipcodes with few data points.\n",
    "    zip_counts = data_clean['zipcode'].value_counts()\n",
    "    low_zips = list(zip_counts[zip_counts <= 20].index)\n",
    "    data_clean.loc[data_clean['zipcode'].isin(low_zips), 'zipcode'] = np.nan\n",
    "\n",
    "    # Dropping any missing values.\n",
    "    data_clean.dropna(subset=['price', 'sqft_living', 'zipcode'], inplace=True)\n",
    "\n",
    "    data_clean = data_clean.astype(int)\n",
    "    # Creating new columns with all numerical variables normalized.\n",
    "    nums = [\n",
    "        'price', 'sqft_living', 'bedrooms', 'bathrooms', 'sqft_lot',\n",
    "        'sqft_basement', 'population', 'density', 'view', 'grade', 'floors'\n",
    "    ]\n",
    "\n",
    "    for i in nums:\n",
    "        data_clean[i +\n",
    "                   '_norm'] = (data_clean[i] -\n",
    "                               data_clean[i].mean()) / (data_clean[i].std())\n",
    "    data_clean = data_clean[[\n",
    "        'price', 'bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot',\n",
    "        'sqft_basement', 'floors', 'grade', 'population', 'density',\n",
    "        'bedrooms_norm', 'bathrooms_norm', 'view', 'waterfront', 'greenbelt',\n",
    "        'nuisance', 'condition', 'yr_built', 'yr_renovated', 'zipcode',\n",
    "        'yr_last_construction', 'sqft_living_norm', 'sqft_lot_norm',\n",
    "        'sqft_basement_norm', 'floors_norm', 'grade_norm', 'population_norm',\n",
    "        'density_norm', 'view_norm'\n",
    "    ]]\n",
    "    return data_clean"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env3)",
   "language": "python",
   "name": "learn-env3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
