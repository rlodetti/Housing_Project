{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-15T23:01:28.417692Z",
     "start_time": "2023-06-15T23:01:26.168886Z"
    }
   },
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-15T23:01:28.453742Z",
     "start_time": "2023-06-15T23:01:28.447119Z"
    }
   },
   "outputs": [],
   "source": [
    "def make_id(name):\n",
    "    \"\"\"\n",
    "    This function combines two columns to make unique ids for each property,\n",
    "    to be used later for merging.\n",
    "    \"\"\"\n",
    "    name['Major'] = name['Major'].astype(str)\n",
    "    name['Minor'] = name['Minor'].astype(str)\n",
    "    name['id'] = name['Major'].str.cat(name['Minor']).astype(int)\n",
    "    name.drop(columns=['Major', 'Minor'], inplace=True)\n",
    "    return name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Original Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-15T23:01:29.683891Z",
     "start_time": "2023-06-15T23:01:29.444725Z"
    }
   },
   "outputs": [],
   "source": [
    "# Importing the original dataset and the list of zipcodes from King County\n",
    "df = pd.read_csv('data/kc_house_data.csv')\n",
    "kings_zips = list(pd.read_csv('data/zips.csv')['0'])\n",
    "\n",
    "# Extracting the zipcode from the address column.\n",
    "df['zipcode'] = df['address'].apply(lambda x: int(x[-20:-15]))\n",
    "df = df[df['zipcode'].isin(kings_zips)]\n",
    "\n",
    "# Dropping duplicate id's.\n",
    "df.drop_duplicates(subset=['id'], inplace=True)\n",
    "\n",
    "# Renaming elements in categorical variables to a binary 0 and 1, or a \n",
    "# numerical ranking.\n",
    "df['greenbelt'] = df['greenbelt'].map({'NO': 0, 'YES': 1})\n",
    "df['nuisance'] = df['nuisance'].map({'NO': 0, 'YES': 1})\n",
    "df['waterfront'] = df['waterfront'].map({'NO': 0, 'YES': 1})\n",
    "df['view'] = df['view'].map({\n",
    "    'NONE': 0,\n",
    "    'POOR': 1,\n",
    "    'FAIR': 2,\n",
    "    'AVERAGE': 3,\n",
    "    'GOOD': 4,\n",
    "    'EXCELLENT': 5\n",
    "})\n",
    "df['condition'] = df['condition'].map({\n",
    "    'Poor': 1,\n",
    "    'Fair': 2,\n",
    "    'Average': 3,\n",
    "    'Good': 4,\n",
    "    'Very Good': 5\n",
    "})\n",
    "df['grade'] = df['grade'].map({\n",
    "    '1 Cabin': 1,\n",
    "    '2 Substandard': 2,\n",
    "    '3 Poor': 3,\n",
    "    '4 Low': 4,\n",
    "    '5 Fair': 5,\n",
    "    '6 Low Average': 6,\n",
    "    '7 Average': 7,\n",
    "    '8 Good': 8,\n",
    "    '9 Better': 9,\n",
    "    '10 Very Good': 10,\n",
    "    '11 Excellent': 11,\n",
    "    '12 Luxury': 12,\n",
    "    '13 Mansion': 13\n",
    "})\n",
    "\n",
    "# Selecting columns to keep and setting the id as the index.\n",
    "keep = [\n",
    "    'id', 'bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot', 'floors',\n",
    "    'waterfront', 'greenbelt', 'nuisance', 'view', 'condition', 'grade',\n",
    "    'sqft_basement', 'yr_built', 'yr_renovated', 'zipcode'\n",
    "]\n",
    "df_clean = df[keep].set_index('id', verify_integrity=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Res Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-15T23:01:52.031219Z",
     "start_time": "2023-06-15T23:01:31.855133Z"
    }
   },
   "outputs": [],
   "source": [
    "# Importing the Residential dataset\n",
    "res = pd.read_csv('data/ResBldg.csv', low_memory=False)\n",
    "\n",
    "make_id(res)  # Making 'id' column.\n",
    "\n",
    "# Ensuring that all of the the zipcodes are in the proper format and filtering\n",
    "# for zipcodes only in King County.\n",
    "to_drop = []\n",
    "for i, j in enumerate(res['ZipCode']):\n",
    "    try:\n",
    "        int(res.iloc[i, 9][:5])\n",
    "    except:\n",
    "        if j in to_drop:\n",
    "            pass\n",
    "        else:\n",
    "            to_drop.append(j)\n",
    "res.loc[res['ZipCode'].isin(to_drop), 'ZipCode'] = np.nan\n",
    "res.dropna(subset=['ZipCode'], inplace=True)\n",
    "res['zipcode'] = res['ZipCode'].apply(lambda x: int(str(x)[:5]))\n",
    "res.loc[~res['zipcode'].isin(kings_zips), 'zipcode'] = np.nan\n",
    "res.dropna(subset=['ZipCode'], inplace=True)\n",
    "\n",
    "# Dropping duplicate id's.\n",
    "res.drop_duplicates(subset=['id'], inplace=True, keep=False)\n",
    "\n",
    "# Renaming columns to match the original dataset.\n",
    "mapping = {\n",
    "    'Address': 'address',\n",
    "    'Stories': 'floors',\n",
    "    'BldgGrade': 'grade',\n",
    "    'SqFtTotLiving': 'sqft_living',\n",
    "    'SqFt1stFloor': 'sqft_1st_floor',\n",
    "    'Bedrooms': 'bedrooms',\n",
    "    'BathFullCount': 'bathrooms',\n",
    "    'YrBuilt': 'yr_built',\n",
    "    'Condition': 'condition',\n",
    "    'SqFtTotBasement': 'sqft_basement',\n",
    "    'YrRenovated': 'yr_renovated'\n",
    "}\n",
    "\n",
    "# Selecting columns to keep and setting the id as the index.\n",
    "keep = [\n",
    "    'id', 'address', 'bathrooms', 'bedrooms', 'condition', 'floors', 'grade',\n",
    "    'sqft_basement', 'sqft_living', 'yr_built', 'yr_renovated', 'zipcode'\n",
    "]\n",
    "res_clean = res.rename(columns=mapping)[keep].set_index('id',\n",
    "                                                        verify_integrity=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parcel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-15T23:02:04.659005Z",
     "start_time": "2023-06-15T23:01:52.036260Z"
    }
   },
   "outputs": [],
   "source": [
    "# Importing the Parcel dataset.\n",
    "parcel = pd.read_csv('data/Parcel.csv', encoding='latin-1', low_memory=False)\n",
    "\n",
    "make_id(parcel)  # Making 'id' column.\n",
    "\n",
    "# Filtering data to only include properties labeled as 'Condominium' or 'Residential'.\n",
    "parcel = parcel[parcel['PropType'].isin(['K', 'R'])]\n",
    "\n",
    "# Combining the information from multiple view columns to create a 'view'\n",
    "# column which includes a view rating from 0 to 5.\n",
    "for i in [\n",
    "        'MtRainier', 'Olympics', 'Cascades', 'Territorial', 'SeattleSkyline',\n",
    "        'PugetSound', 'LakeWashington', 'LakeSammamish', 'SmallLakeRiverCreek',\n",
    "        'OtherView'\n",
    "]:\n",
    "    parcel.loc[parcel[i] > 0, i] = 1\n",
    "    parcel.loc[parcel[i] == 0, i] = 0\n",
    "parcel['total_views'] = parcel['MtRainier'] + parcel['Olympics'] + parcel[\n",
    "    'Cascades'] + parcel['Territorial'] + parcel['SeattleSkyline'] + parcel[\n",
    "        'PugetSound'] + parcel['LakeWashington'] + parcel[\n",
    "            'LakeSammamish'] + parcel['SmallLakeRiverCreek'] + parcel[\n",
    "                'OtherView']\n",
    "parcel['view'] = parcel['total_views'].map({\n",
    "    0: 0,\n",
    "    1: 2,\n",
    "    2: 3,\n",
    "    3: 4,\n",
    "    4: 5,\n",
    "    5: 5,\n",
    "    6: 5,\n",
    "    7: 5,\n",
    "    8: 5\n",
    "})\n",
    "\n",
    "# Renaming elements in categorical variables to a binary 0 and 1\n",
    "parcel.loc[parcel['WfntLocation'] > 0, 'waterfront'] = 1\n",
    "parcel.loc[parcel['WfntLocation'] == 0, 'waterfront'] = 0\n",
    "parcel['nuisance'] = 0\n",
    "parcel.loc[parcel['PowerLines'] == 'Y', 'nuisance'] = 1\n",
    "parcel.loc[parcel['TrafficNoise'] > 0, 'nuisance'] = 1\n",
    "parcel.loc[parcel['AirportNoise'] != 0, 'nuisance'] = 1\n",
    "parcel.loc[parcel['OtherNuisances'] == 'Y', 'nuisance'] = 1\n",
    "parcel['greenbelt'] = parcel['AdjacentGreenbelt'].map({'N': 0, 'Y': 1})\n",
    "\n",
    "# Dropping duplicate id's.\n",
    "parcel.drop_duplicates(subset=['id'], inplace=True, keep=False)\n",
    "\n",
    "# Renaming column to match the original dataset.\n",
    "mapping = {'SqFtLot': 'sqft_lot'}\n",
    "\n",
    "# Selecting columns to keep and setting the id as the index.\n",
    "keep = ['id', 'greenbelt', 'nuisance', 'sqft_lot', 'view', 'waterfront']\n",
    "parcel_clean = parcel.rename(columns=mapping)[keep].set_index(\n",
    "    'id', verify_integrity=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-15T23:02:37.496693Z",
     "start_time": "2023-06-15T23:02:04.662379Z"
    }
   },
   "outputs": [],
   "source": [
    "# Importing the Sales dataset.\n",
    "sales = pd.read_csv('data/RP_Sales.csv', encoding='latin-1', low_memory=False)\n",
    "\n",
    "# In this case, the sales data had many id's in an inproper format. This code\n",
    "# cleans and filter the ids\n",
    "sales['Major'] = sales['Major'].astype(str)\n",
    "sales['Minor'] = sales['Minor'].astype(str)\n",
    "sales['id'] = sales['Major'].str.cat(sales['Minor'])\n",
    "to_drop = []\n",
    "for i, j in enumerate(sales['id']):\n",
    "    try:\n",
    "        int(j)\n",
    "    except:\n",
    "        to_drop.append(j)\n",
    "sales.loc[sales['id'].isin(to_drop), 'id'] = np.nan\n",
    "sales.dropna(subset=['id'], inplace=True)\n",
    "sales['id'] = sales['id'].astype(int)\n",
    "\n",
    "# Converting and extracting the year from the date column.\n",
    "sales['date'] = pd.to_datetime(sales['DocumentDate'])\n",
    "sales['year'] = sales['date'].apply(lambda x: x.year)\n",
    "\n",
    "# Selecting and renaming the columns.\n",
    "keep = ['id', 'date', 'price']\n",
    "mapping = {'SalePrice': 'price'}\n",
    "\n",
    "cond1 = sales['id'] != 0\n",
    "\n",
    "# Filter to include condominiums, apartments, residential buildings, and mobile homes.\n",
    "cond2 = sales['PrincipalUse'].isin([2, 4, 6, 8])\n",
    "\n",
    "cond3 = sales['year'] > 2020\n",
    "sales2 = sales[cond1 & cond2 & cond3].rename(columns=mapping)[keep]\n",
    "\n",
    "# Only keeping the most recent sale of a property.\n",
    "sales3 = sales2.sort_values('date',\n",
    "                            ascending=False).drop_duplicates(subset=['id'],\n",
    "                                                             keep='first')\n",
    "\n",
    "sales_clean = sales3.sort_values('id').set_index('id', verify_integrity=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Open Data Soft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-15T23:02:37.522051Z",
     "start_time": "2023-06-15T23:02:37.499500Z"
    }
   },
   "outputs": [],
   "source": [
    "# Importing the Open Data Soft dataset.\n",
    "ods = pd.read_csv('data/open_data_soft.csv',sep=';')\n",
    "\n",
    "# Selecting and renaming the columns.\n",
    "ods = ods[['Zip Code','Population','Density']]\n",
    "ods.rename(columns={'Zip Code':'zipcode','Population':'population','Density':'density'},inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-15T23:44:13.822726Z",
     "start_time": "2023-06-15T23:44:12.129668Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18385"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exclude outliers using industry knowledge.\n",
    "data_clean = data_clean[data_clean['bedrooms'].between(1, 6)]\n",
    "data_clean = data_clean[data_clean['bathrooms'].between(1, 6)]\n",
    "data_clean = data_clean[(data_clean['sqft_lot'] < 43560)]\n",
    "data_clean = data_clean[data_clean['sqft_basement'] < 4000]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-16T09:45:26.510875Z",
     "start_time": "2023-06-16T09:45:24.442849Z"
    }
   },
   "outputs": [],
   "source": [
    "# 3\n",
    "# Merging all of the above datasets.\n",
    "data_clean = sales_clean.join(df_clean)\n",
    "data_clean.update(res_clean)\n",
    "data_clean.update(parcel_clean)\n",
    "data_clean = data_clean.merge(ods, how='left', on='zipcode')\n",
    "\n",
    "# Filtering and remoming all missing values.\n",
    "data_clean.loc[data_clean['price'] <= 0, 'price'] = np.nan\n",
    "data_clean.loc[data_clean['sqft_living'] <= 0, 'sqft_living'] = np.nan\n",
    "data_clean.dropna(subset=['price','sqft_living','zipcode'],inplace=True)\n",
    "\n",
    "# Excluding outliers by selecting for the middle 95% price and sqft_living data.\n",
    "data_clean['price_nlog'] = (np.log(data_clean['price']) - np.log(\n",
    "    data_clean['price']).mean()) / np.log(data_clean['price']).std()\n",
    "data_clean.loc[(data_clean['price_nlog'] > 3) |\n",
    "               (data_clean['price_nlog'] < -3), 'price_nlog'] = np.nan\n",
    "data_clean.dropna(subset=['price_nlog'], inplace=True)\n",
    "\n",
    "# Creating a column to calculate the last year construction was done on the property.\n",
    "data_clean['yr_last_construction'] = data_clean['yr_built']\n",
    "data_clean['yr_last_construction'].update(\n",
    "    data_clean['yr_renovated'][data_clean['yr_renovated'] != 0])\n",
    "\n",
    "data_clean = data_clean[data_clean['bedrooms'].between(1, 6)]\n",
    "data_clean = data_clean[data_clean['bathrooms'].between(1, 6)]\n",
    "data_clean = data_clean[(data_clean['sqft_lot'] < 43560)]\n",
    "data_clean = data_clean[data_clean['sqft_basement'] < 4000]\n",
    "\n",
    "# Excluding zipcodes with few data points.\n",
    "zip_counts = data_clean['zipcode'].value_counts()\n",
    "low_zips = list(zip_counts[zip_counts<=20].index)\n",
    "data_clean.loc[data_clean['zipcode'].isin(low_zips), 'zipcode'] = np.nan\n",
    "\n",
    "# Dropping any missing values.\n",
    "data_clean.dropna(subset=['price','sqft_living','zipcode'],inplace=True)\n",
    "\n",
    "# Creating new columns with all numerical variables normalized.\n",
    "nums = [\n",
    "    'price', 'sqft_living', 'bedrooms', 'bathrooms', 'sqft_lot',\n",
    "    'sqft_basement', 'population', 'density', 'view', 'grade', 'floors'\n",
    "]\n",
    "for i in nums:\n",
    "    data_clean[i + '_norm'] = (data_clean[i] -\n",
    "                               data_clean[i].mean()) / (data_clean[i].std())\n",
    "\n",
    "data_clean.to_csv('data/data_clean3.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env3)",
   "language": "python",
   "name": "learn-env3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
